# 机器学习

!!! abstract "摘要"

    机器学习算法可以分为两大类：
    
    - 监督学习：
    给定训练数据包含输入与相应输出，算法的目标是从训练数据中获得隐含的函数关系，并据此将新的测试输入映射为正确的输出。监督学习可以分为以下两种：
        - 分类：
        输出是离散的；
        - 回归：
        输出是连续的；

    - 无监督学习：
    给定数据通常由高维向量的输入组成，算法的目标是在未标记的数据中挖掘隐藏的统计结构，通过学习构建一种紧凑的或对后续分析有用的统计模型。

    我们已经讨论了两种主要的无监督学习技术（PCA和ICA），下文将继续讨论常见的监督学习技术。

    - 基于EEG、ECoG、fMRI、fNRI等的BCI主要利用分类产生离散的控制输出信号；
    - 基于神经元记录的BCI主要利用回归产生连续的输出信号；

## 分类技术

### 二分类

分类器的任务是为 $p$ 维的特征向量 $x$ 分配类别标签 $y\in Y$ 。最简单的情况在两类（标记为 $-1$ 和 $+1$）之间进行区分，即二分类。

二分类问题的关键在于通过训练数据计算找到一个边界，以使新的数据能被正确分类。

#### 线性判别分析

线性判别分析（LDA）是一个线性二分类器，将 $p$ 维输入向量 $x$ 映射到一个超平面，该超平面将输入空间划分为两个半空间，超平面公式如下：

$$
g(x) = w^T x + w_0 = 0
$$

边界由超平面的法向量 $w$ 和阈值 $w_0$ 来表示，它们由被标记的训练数据决定。

对于新的输入向量 $x\in X^p$ ，通过如下计算对其进行二分类：

$$
y = sign(w^T x + w_0)
$$

img

**LDA实现简单、运算速度较快，已经成为BCI研究中常用的分类器**。尽管由于在LDA的推导中做了**强假设**，诸如非高斯分布、异常值、噪声等因素会降低LDA的性能，但总体上LDA能产生好的分类结果。

LDA至少有以下两种变式：
- 正则化线性判别分析（RDA）：
将LDA的协方差用正则化形式取代，以提升泛化能力和避免过度拟合。
- 二次判别分析（QDA）：
与LDA的不同之处在于QDA允许两类有不同的协方差矩阵。

#### 神经网络和感知器

神经网络（ANN）受生物学中神经网络的启发，力图重建大脑网络的适应能力，以一种强健的方式对输入数据进行分类。

一个著名的例子是感知器（及多层感知器）。单层感知器计算一个超平面（类似于LDA）：

$$
w^T x + w_0 = 0
$$

$$
y = sign(w^T x + w_0)
$$

其中，向量 $w$ 表示连接输入与神经元的“突触权值”， $-w_0$ 表示神经元的放电阈值。

对此有一个“神经”视角的解释：**神经元的输出是基于对输入的加权和计算，以及对加权和与阈值的比较**。（这可视为产生锋电位阈值模型的一种简化形式）

感知器和LDA不同之处在于权值和阈值参数如何适应输入。受生物学启发，感知器以在线的方式调节其参数：给定一个输入 $x$ 和期望的输出 $y^d$ ，如果输出误差 $y-y^d$ 为正，那么正输入的权值减小，负输入的权值增大，并且阈值增大。**这种“学习”规则的净效应是减少将来类似输入所产生的输出误差**。

**多层感知器是感知器的非线性推广**，其使用`sigmoid`软阈值非线性函数，而不是使用硬阈值非线性函数来表示它们的神经单元：

$$
y = sigmoid(w^T x + w_0)
$$

sigmoid函数的输出是0~1之间的数字，其值接近于0则表示属于类型1，接近于1则表示属于类型2 。关于sigmoid以及神经网络的更多内容，参见下文“神经网络与反向传播算法”部分。

#### 支持向量机

神经网络很强大，但是存在对训练数据过度拟合，导致其泛化能力变差的问题。较新的技术支持向量机（SVM）通常比神经网络更受青睐，成为众多BCI选择的分类算法。

LDA和感知器通过选择超平面来分离两类，而被选择的超平面有无数种合适的可能，可以证明在这些超平面中，**选择两类之间距离最大的超平面**能获得最好的泛化能力。SVM就是这样的一个分类器。

img

**线性SVM已经成功用于大量BCI应用**。线性SVM就不能解决问题的情况下，可利用**核技巧**（kernel trick）来有效实现数据的非线性映射，将数据映射到更高维的空间中，使数据线性可分。（BCI中最常用的核是高斯核或径向基函数）

### 集成分类技术

分类的集成方法结合多个分类器的输出，形成一个比任意单一分类器的泛化能力更好的综合分类器。

#### bagging和随机森林

bagging是一种最简单的集成学习方法，可概括如下：

1. 对给定数据集进行有放回抽样（意味着同一个训练集中样本可能重复），产生m个新的训练集；
2. 训练m个分类器，每个分类器对应一个新产生的训练集；
3. 通过m个分类器对新的输入进行分类，选择获得最多“投票”的类别；

考虑 $N^{'} = N$ 这一典型情况（$N^{'}$ 指的是每个训练集的抽样样本数），这样产生的样本集被称作`bootstrap样本`。

随机森林可能是当今最流行的bagging技术，其名字源于它们由许多**决策树分类器**组成。在随机森林中，输入向量首先森林中的每一棵树，每棵树预测一个输出类别，森林选择获得树投票最多的类别作为其输出。

由于随机森林**在具有大量输入变量的大数据集上表现良好**，使得它近年来愈加流行。但是随机森林**在BCI中的应用依然相对较少**。

#### boosting

boosting寻找一系列分类器，这些分类器**给予预测错误的数据点的权重高于预测正确的**，由此可以找到新的分类器，对当前分类器不能很好分类的数据点能进行更好的分类。

boosting与bagging不同之处在于它的**新分类器依赖于之前分类器的表现**。

boosting对解决弱分类器问题（这些弱分类器的表现可能仅比随机的结果好一些）非常有用，可以**基于弱分类器构建出强分类器**，以提高准确度。

或许最有名的boosting算法是`AdaBoost`。

### 多分类

上述讨论的分类器都是二分类的，而在BCI应用中，期望的输出信号个数通常大于二，这就需要使用多分类方法。

多分类也可以通过二分类器结合的方法得到：
- 训练若干个二分类器，并使用多数投票机制，每个二分类器用于一个两类组合；
- 采取一对多的策略，对于每一类，训练一个独立的分类器来分离该类数据和其他类别的数据；

#### 最近邻

最近邻（NN）是最简单的多分类技术之一，输入被简单指定为其最近邻的类别。

NN分类存在的一个问题是它对噪声和异常值很敏感，这种技术可以通过使用k-最近邻（k-NN）来变得更加稳健。在k-NN中，输入被指定为k个最近邻中最普遍的一种类别。

img

k-NN技术存在的一个潜在问题是它偏向于训练集中最多样本所属的类别，其有一种变式，把距离和类别同时纳入考虑，可以解决这一问题。

#### 学习矢量化



#### 朴素贝叶斯分类器

### 分类性能的评估

## 回归方法