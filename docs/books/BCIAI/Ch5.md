# 机器学习

!!! abstract "摘要"

    机器学习算法可以分为两大类：
    - 监督学习：
    给定训练数据包含输入与相应输出，算法的目标是从训练数据中获得隐含的函数关系，并据此将新的测试输入映射为正确的输出。监督学习可以分为以下两种：
        - 分类：
        输出是离散的；
        - 回归：
        输出是连续的；

    - 无监督学习：
    给定数据通常由高维向量的输入组成，算法的目标是在未标记的数据中挖掘隐藏的统计结构，通过学习构建一种紧凑的或对后续分析有用的统计模型。

    我们已经讨论了两种主要的无监督学习技术（PCA和ICA），下文将继续讨论常见的监督学习技术。

    - 基于EEG、ECoG、fMRI、fNRI等的BCI主要利用分类产生离散的控制输出信号；
    - 基于神经元记录的BCI主要利用回归产生连续的输出信号；

## 分类技术

### 二分类

分类器的任务是为 $p$ 维的特征向量 $x$ 分配类别标签 $y\in Y$ 。最简单的情况在两类（标记为 $-1$ 和 $+1$）之间进行区分，即二分类。

二分类问题的关键在于通过训练数据计算找到一个边界，以使新的数据能被正确分类。

#### 线性判别分析

线性判别分析（LDA）是一个线性二分类器，将 $p$ 维输入向量 $x$ 映射到一个超平面，该超平面将输入空间划分为两个半空间，超平面公式如下：

$$
g(x) = w^T x + w_0 = 0
$$

边界由超平面的法向量 $w$ 和阈值 $w_0$ 来表示，他们由被标记的训练数据决定。

对于新的输入向量 $x\in X^p$ ，通过如下计算对其进行二分类：

$$
y = sign(w^T x + w_0)
$$

img

**LDA实现简单、运算速度较快，已经成为BCI研究中常用的分类器**。尽管由于在LDA的推导中做了**强假设**，诸如非高斯分布、异常值、噪声等因素会降低LDA的性能，但总体上LDA能产生好的分类结果。

LDA至少有以下两种变式：
- 正则化线性判别分析（RDA）：
将LDA的协方差用正则化形式取代，以提升泛化能力和避免过度拟合。
- 二次判别分析（QDA）：
与LDA的不同之处在于QDA允许两类有不同的协方差矩阵。

#### 神经网络和感知器

神经网络（ANN）受生物学中神经网络的启发，力图重建大脑网络的适应能力，以一种强健的方式对输入数据进行分类。

一个著名的例子是感知器（及多层感知器）。单层感知器计算一个超平面（类似于LDA）：

$$
w^T x + w_0 = 0
$$

$$
y = sign(w^T x + w_0)
$$

其中，向量 $w$ 表示连接输入与神经元的“突触权值”， $-w_0$ 表示神经元的放电阈值。

对此有一个“神经”视角的解释：**神经元的输出是基于对输入的加权和计算，以及对加权和与阈值的比较**。（这可视为产生锋电位阈值模型的一种简化形式）

感知器和LDA不同之处在于权值和阈值参数如何适应输入。受生物学启发，感知器以在线的方式调节其参数：给定一个输入 $x$ 和期望的输出 $y^d$ ，如果输出误差 $y-y^d$ 为正，那么正输入的权值减小，负输入的权值增大，并且阈值增大。**这种“学习”规则的净效应是减少将来类似输入所产生的输出误差**。

**多层感知器是感知器的非线性推广**，其使用`sigmoid`软阈值非线性函数，而不是使用硬阈值非线性函数来表示他们的神经单元：

$$
y = sigmoid(w^T x + w_0)
$$

sigmoid函数的输出是0~1之间的数字，其值接近于0则表示属于类型1，接近于1则表示属于类型2 。关于sigmoid以及神经网络的更多内容，参见下文“神经网络与反向传播算法”部分。

#### 支持向量机

神经网络很强大，但是存在对训练数据过度拟合，导致其泛化能力变差的问题。较新的技术支持向量机（SVM）通常比神经网络更受青睐，成为众多BCI选择的分类算法。

LDA和感知器通过选择超平面来分离两类，而被选择的超平面有无数种合适的可能，可以证明在这些超平面中，**选择两类之间距离最大的超平面**能获得最好的泛化能力。SVM就是这样的一个分类器。

img

**线性SVM已经成功用于大量BCI应用**。线性SVM就不能解决问题的情况下，可利用**核技巧**（kernel trick）来有效实现数据的非线性映射，将数据映射到更高维的空间中，使数据线性可分。（BCI中最常用的核是高斯核或径向基函数）

### 集成分类技术

分类的集成方法结合多个分类器的输出，形成一个比任意单一分类器的泛化能力更好的综合分类器。

ss

#### bagging和随机森林

#### boosting

## 回归方法