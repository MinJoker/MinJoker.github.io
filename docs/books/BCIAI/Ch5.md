# 机器学习

!!! abstract "摘要"

    机器学习算法可以分为两大类：
    
    - 监督学习：
    给定训练数据包含输入与相应输出，算法的目标是从训练数据中获得隐含的函数关系，并据此将新的测试输入映射为正确的输出。监督学习可以分为以下两种：
        - 分类：
        输出是离散的；
        - 回归：
        输出是连续的；

    - 无监督学习：
    给定数据通常由高维向量的输入组成，算法的目标是在未标记的数据中挖掘隐藏的统计结构，通过学习构建一种紧凑的或对后续分析有用的统计模型。

    我们已经讨论了两种主要的无监督学习技术（PCA和ICA），下文将继续讨论常见的监督学习技术。

    - 基于EEG、ECoG、fMRI、fNRI等的BCI主要利用分类产生离散的控制输出信号；
    - 基于神经元记录的BCI主要利用回归产生连续的输出信号；

## 分类技术

### 二分类

分类器的任务是为 $p$ 维的特征向量 $x$ 分配类别标签 $y\in Y$ 。最简单的情况在两类（标记为 $-1$ 和 $+1$）之间进行区分，即二分类。

二分类问题的关键在于通过训练数据计算找到一个边界，以使新的数据能被正确分类。

#### 线性判别分析

线性判别分析（LDA）是一个线性二分类器，将 $p$ 维输入向量 $x$ 映射到一个超平面，该超平面将输入空间划分为两个半空间，超平面公式如下：

$$
g(x) = w^T x + w_0 = 0
$$

边界由超平面的法向量 $w$ 和阈值 $w_0$ 来表示，它们由被标记的训练数据决定。

对于新的输入向量 $x\in X^p$ ，通过如下计算对其进行二分类：

$$
y = sign(w^T x + w_0)
$$

![二分类的线性判别分析](https://raw.githubusercontent.com/MinJoker/ImageHost/main/books/BCIAI/7.jpg "二分类的线性判别分析")

**LDA实现简单、运算速度较快，已经成为BCI研究中常用的分类器**。尽管由于在LDA的推导中做了**强假设**，诸如非高斯分布、异常值、噪声等因素会降低LDA的性能，但总体上LDA能产生好的分类结果。

LDA至少有以下两种变式：
- 正则化线性判别分析（RDA）：
将LDA的协方差用正则化形式取代，以提升泛化能力和避免过度拟合。
- 二次判别分析（QDA）：
与LDA的不同之处在于QDA允许两类有不同的协方差矩阵。

#### 神经网络与感知器

神经网络（ANN）受生物学中神经网络的启发，力图重建大脑网络的适应能力，以一种强健的方式对输入数据进行分类。

一个著名的例子是感知器（及多层感知器）。单层感知器计算一个超平面（类似于LDA）：

$$
w^T x + w_0 = 0
$$

$$
y = sign(w^T x + w_0)
$$

其中，向量 $w$ 表示连接输入与神经元的“突触权值”， $-w_0$ 表示神经元的放电阈值。

对此有一个“神经”视角的解释：**神经元的输出是基于对输入的加权和计算，以及对加权和与阈值的比较**。（这可视为产生锋电位阈值模型的一种简化形式）

感知器和LDA不同之处在于权值和阈值参数如何适应输入。受生物学启发，感知器以在线的方式调节其参数：给定一个输入 $x$ 和期望的输出 $y^d$ ，如果输出误差 $y-y^d$ 为正，那么正输入的权值减小，负输入的权值增大，并且阈值增大。**这种“学习”规则的净效应是减少将来类似输入所产生的输出误差**。

**多层感知器是感知器的非线性推广**，其使用`sigmoid`软阈值非线性函数，而不是使用硬阈值非线性函数来表示它们的神经单元：

$$
y = sigmoid(w^T x + w_0)
$$

sigmoid函数的输出是0~1之间的数字，其值接近于0则表示属于类型1，接近于1则表示属于类型2 。关于sigmoid以及神经网络的更多内容，参见下文“神经网络与反向传播算法”部分。

#### 支持向量机

神经网络很强大，但是存在对训练数据过度拟合，导致其泛化能力变差的问题。较新的技术支持向量机（SVM）通常比神经网络更受青睐，成为众多BCI选择的分类算法。

LDA和感知器通过选择超平面来分离两类，而被选择的超平面有无数种合适的可能，可以证明在这些超平面中，**选择两类之间距离最大的超平面**能获得最好的泛化能力。SVM就是这样的一个分类器。

![支持向量机](https://raw.githubusercontent.com/MinJoker/ImageHost/main/books/BCIAI/8.jpg "支持向量机")

**线性SVM已经成功用于大量BCI应用**。线性SVM就不能解决问题的情况下，可利用**核技巧**（kernel trick）来有效实现数据的非线性映射，将数据映射到更高维的空间中，使数据线性可分。（BCI中最常用的核是高斯核或径向基函数）

### 集成分类技术

分类的集成方法结合多个分类器的输出，形成一个比任意单一分类器的泛化能力更好的综合分类器。

#### bagging与随机森林

bagging是一种最简单的集成学习方法，可概括如下：

1. 对给定数据集进行有放回抽样（意味着同一个训练集中样本可能重复），产生m个新的训练集；
2. 训练m个分类器，每个分类器对应一个新产生的训练集；
3. 通过m个分类器对新的输入进行分类，选择获得最多“投票”的类别；

考虑 $N^{'} = N$ 这一典型情况（$N^{'}$ 指的是每个训练集的抽样样本数），这样产生的样本集被称作`bootstrap样本`。

随机森林可能是当今最流行的bagging技术，其名字源于它们由许多**决策树分类器**组成。在随机森林中，输入向量首先森林中的每一棵树，每棵树预测一个输出类别，森林选择获得树投票最多的类别作为其输出。

由于随机森林**在具有大量输入变量的大数据集上表现良好**，使得它近年来愈加流行。但是随机森林**在BCI中的应用依然相对较少**。

#### boosting

boosting寻找一系列分类器，这些分类器**给予预测错误的数据点的权重高于预测正确的**，由此可以找到新的分类器，对当前分类器不能很好分类的数据点能进行更好的分类。

boosting与bagging不同之处在于它的**新分类器依赖于之前分类器的表现**。

boosting对解决弱分类器问题（这些弱分类器的表现可能仅比随机的结果好一些）非常有用，可以**基于弱分类器构建出强分类器**，以提高准确度。

或许最有名的boosting算法是`AdaBoost`。

### 多分类

上述讨论的分类器都是二分类的，而在BCI应用中，期望的输出信号个数通常大于二，这就需要使用多分类方法。

多分类也可以通过二分类器结合的方法得到：
- 训练若干个二分类器，并使用多数投票机制，每个二分类器用于一个两类组合；
- 采取一对多的策略，对于每一类，训练一个独立的分类器来分离该类数据和其他类别的数据；

#### 最近邻

最近邻（NN）是最简单的多分类技术之一，输入被简单指定为其最近邻的类别。

NN分类存在的一个问题是它对噪声和异常值很敏感，这种技术可以通过使用**k-最近邻**（k-NN）来变得更加稳健。在k-NN中，输入被指定为k个最近邻中最普遍的一种类别。

![k-最近邻](https://raw.githubusercontent.com/MinJoker/ImageHost/main/books/BCIAI/9.jpg "k-最近邻")

k-NN技术存在的一个潜在问题是它偏向于训练集中最多样本所属的类别，其有一种变式，把距离和类别同时纳入考虑，可以解决这一问题。

#### 学习矢量量化

学习矢量量化（LVQ）中，分类基于标签 $Y_i\in [1,\cdots ,N_y]$ 标记的特征向量 $\{m_i, Y_i \}_{i=1}^N$（也称为码本向量，lodebook vector）构成的小集合。新样本的分类通过将与它距离最近的码本向量 $m_k$ 的标签 $Y_k$ 赋予样本实现。

LVQ中的每个码本向量的贡献是相等的，但在BCI中更常见的情况是需要对不同特征作加权处理，这就需要使用LVQ的一种优化，即**区分敏感LVQ**（DSLVQ），在计算“距离”时进行加权。

#### 朴素贝叶斯分类器

朴素贝叶斯分类器是一种基于强独立性（朴素）假设贝叶斯准则的概率分类器。假设目标是要根据输入计算得到的大量特征 $F_1, F_2, \cdots , F_n$ 确定特定输入的所属类别（从 $N$ 种可能的类别中选择）。实现这一目标的一种方式是选择具有最大后验概率的类别 $i$ ：

$$
\begin{aligned}
P(C = i \vert F_1, F_2, \cdots , F_n) & = \frac{P(C=i) P(F_1, F_2, \cdots , F_n | C=i)}{P(F_1, F_2, \cdots , F_n)} \\[4ex]
& = \frac{P(C=i) P(F_1 | C=i)P(F_2 | C=i)\cdots P(F_n | C=i)}{P(F_1,F_2,\cdots ,F_n)} \\[4ex]
& \propto P(C=i) P(F_1 | C=i)P(F_2 | C=i)\cdots P(F_n | C=i)
\end{aligned}
$$

### 分类性能的评估

#### 混淆矩阵与ROC曲线：

对于 $N_y \times N_y$ 的混淆矩阵 $M$ ，行表示真实的类标签，列表示分类器的输出。$M$ 的对角元素表示正确分类的样本，非对角元素 $M_{ij}$ 给出了有多少类 $i$ 的样本被误分类为 $j$ 。

两类问题的混淆矩阵如下：

$$
\begin{array}{lll}
{\qquad}&{正}&{负}\\
\hline
{正}&{真阳性（TP）}&{假阴性（FN）}\\
{负}&{假阳性（FP）}&{真阴性（TN）}
\end{array}
$$

ROC曲线（“受试者操作特征”曲线）是真阳性比例和假阳性比例的对比图，能**反映敏感性与特异性之间的关系**。横坐标为假阳性率（$1-$特异性），纵坐标为真阳性率（敏感度）。

![ROC空间](https://raw.githubusercontent.com/MinJoker/ImageHost/main/books/BCIAI/10.jpg "ROC空间")

根据曲线的位置，把整个图分成两部分，曲线下方部分的面积（AUC）越大，表示预测准确性越高；曲线越接近左上角，预测准确性越高。

![ROC曲线](https://raw.githubusercontent.com/MinJoker/ImageHost/main/books/BCIAI/11.jpg "ROC曲线")

#### 分类正确率与Kappa系数

分类正确率（ACC）定义为正确分类的样本数与样本总数的比值。定义错误率 $err = 1 - ACC$ 。当每一类的样本数目相同时，机会水平（随机） $ACC_0 = 1/N_Y$ ,其中 $N_Y$ 表示类别数。

kappa系数定义如下：

$$
\kappa = \frac{ACC - ACC_0}{1 - ACC_0}
$$

kappa系数与每类样本的数目和类别数无关。$\kappa = 0$ 意味着机会水平的表现，$\kappa = 1$ 意味着最好的分类，$\kappa < 0$ 意味着分类性能差于机会水平。

#### 信息传输率

为比较BCI性能，同时考虑BCI的正确率和速度是非常重要，可以利用信息论的概念，根据比特率或者信息传输率（ITR）来量化BCI的性能。

$$
B = log_2 (N) + P log_2 (P) + (1-P) log_2 (1-P) / (N-1)
$$

现实情况下虽然不一定总能满足推导 $B$ 所做得假设条件，但是 $B$ 提供了一个能够获得的性能上限。

#### 交叉验证

交叉验证用于对错误率 $err$ 的估计。一种方法是将给定的输入数据集简单地划分为两个子集，一个子集用于训练，另一个子集用于测试（hand out 方法）。但是这种方法对数据怎样划分很敏感。

一种更复杂的方法是**K折交叉验证**：将数据集划分为K个维数大致相同的子集，其中K-1个子集用于训练分类器，剩下的子集用于训练。对分类器进行K次训练和测试，产生K个不同的错误率，总错误率可以通过求平均得到：

$$
err = \frac{1}{K} \sum_{k=1}^K err_k
$$

在很多的应用中，一般将数据集划分为三个子集：一个是用于确定分类器参数的训练子集，一个是用于调整分类器参数的验证子集，一个是用于报告优化分类器性能的测试子集。虽然这些过程计算量大，但它们对于提高分类器的泛化能力起着重要作用。

## 回归方法

### 线性回归

线性回归假设产生数据的向量函数是线性的。为了更好地说明线性回归，考虑输入 $u$ 是 $K$ 维向量、输出 $v$ 是标量值的特殊情况，于是输出由线性函数给出：

$$
v = \sum_{i=1}^K w_i u_i = w^T u
$$

上式中，$w$ 是需要由训练数据来确定的“权”向量或线性滤波器。线性最小二乘回归寻找能减少所有训练样本的输出误差平方和的权向量 $w$：

$$
\begin{aligned}
E(w) &= \sum_m (d^m - v^m) ^2 \\[2ex]
&= \Vert d-Uw \Vert ^2
\end{aligned}
$$

上式中，$d$ 是训练输出向量，$U$ 是输入矩阵，矩阵的行是来自训练集的输入向量 $u$。为减小误差，对关于 $w$ 的 $E$ 求导，并将求导结果置为 $0$，得到：

$$
2 \cdot U^T (d-Uw) = 0
$$

$$
U^T U w = U^T d
$$

$$
w = (U^T U) ^{-1} U^T d
$$

估计权向量的上述方法有时也被称为广义逆法（矩阵 $(U^T U)^{-1} U^T$ 是“伪逆”的）。

**线性回归已经被证明在很多侵入式BCI中非常有效**。它速度快且易于计算，主要缺点是在某些应用中对模型过于简化，此外，它也没有在输出中提供任何关于不确定性的估计。

### 神经网络与反向传播算法

神经网络是用于**非线性函数逼近**的流行算法。

ss