# 机器学习

!!! abstract "摘要"

    机器学习算法可以分为两大类：
    
    - 监督学习：
    给定训练数据包含输入与相应输出，算法的目标是从训练数据中获得隐含的函数关系，并据此将新的测试输入映射为正确的输出。监督学习可以分为以下两种：
        - 分类：
        输出是离散的，将大脑活动映射为一组给定类别中的一类；
        - 回归：
        输出是连续的，将神经活动映射为连续的输出信号；

    - 无监督学习：
    给定数据通常由高维向量的输入组成，算法的目标是在未标记的数据中挖掘隐藏的统计结构，通过学习构建一种紧凑的或对后续分析有用的统计模型。

    构建一个BCI一般需要将大脑信号映射为合适的控制信号，这通常利用分类技术或者回归技术实现。
    
    我们已经讨论了两种主要的无监督学习技术（PCA和ICA），下文将继续讨论常见的监督学习技术。

    - 基于EEG、ECoG、fMRI、fNRI等的BCI主要利用分类产生离散的控制输出信号；
    - 基于神经元记录的BCI主要利用回归产生连续的输出信号；

## 分类技术

### 二分类

分类器的任务是为 $p$ 维的特征向量 $x$ 分配类别标签 $y\in Y$ 。最简单的情况在两类（标记为 $-1$ 和 $+1$）之间进行区分，即二分类。

二分类问题的关键在于通过训练数据计算找到一个边界，以使新的数据能被正确分类。

#### 线性判别分析

线性判别分析（LDA）是一个线性二分类器，将 $p$ 维输入向量 $x$ 映射到一个超平面，该超平面将输入空间划分为两个半空间，超平面公式如下：

$$
g(x) = w^T x + w_0 = 0
$$

边界由超平面的法向量 $w$ 和阈值 $w_0$ 来表示，它们由被标记的训练数据决定。

对于新的输入向量 $x\in X^p$ ，通过如下计算对其进行二分类：

$$
y = sign(w^T x + w_0)
$$

![二分类的线性判别分析](https://raw.githubusercontent.com/MinJoker/ImageHost/main/books/BCIAI/7.jpg "二分类的线性判别分析")

**LDA实现简单、运算速度较快，已经成为BCI研究中常用的分类器**。尽管由于在LDA的推导中做了**强假设**，诸如非高斯分布、异常值、噪声等因素会降低LDA的性能，但总体上LDA能产生好的分类结果。

LDA至少有以下两种变式：
- 正则化线性判别分析（RDA）：
将LDA的协方差用正则化形式取代，以提升泛化能力和避免过度拟合。
- 二次判别分析（QDA）：
与LDA的不同之处在于QDA允许两类有不同的协方差矩阵。

#### 神经网络与感知器

神经网络（ANN）受生物学中神经网络的启发，力图重建大脑网络的适应能力，以一种强健的方式对输入数据进行分类。

一个著名的例子是感知器（及多层感知器）。单层感知器计算一个超平面（类似于LDA）：

$$
w^T x + w_0 = 0
$$

$$
y = sign(w^T x + w_0)
$$

其中，向量 $w$ 表示连接输入与神经元的“突触权值”， $-w_0$ 表示神经元的放电阈值。

对此有一个“神经”视角的解释：**神经元的输出是基于对输入的加权和计算，以及对加权和与阈值的比较**。（这可视为产生锋电位阈值模型的一种简化形式）

感知器和LDA不同之处在于权值和阈值参数如何适应输入。受生物学启发，感知器以在线的方式调节其参数：给定一个输入 $x$ 和期望的输出 $y^d$ ，如果输出误差 $y-y^d$ 为正，那么正输入的权值减小，负输入的权值增大，并且阈值增大。**这种“学习”规则的净效应是减少将来类似输入所产生的输出误差**。

**多层感知器是感知器的非线性推广**，其使用`sigmoid`软阈值非线性函数，而不是使用硬阈值非线性函数来表示它们的神经单元：

$$
y = sigmoid(w^T x + w_0)
$$

sigmoid函数的输出是0~1之间的数字，其值接近于0则表示属于类型1，接近于1则表示属于类型2 。关于sigmoid以及神经网络的更多内容，参见下文“神经网络与反向传播算法”部分。

#### 支持向量机

神经网络很强大，但是存在对训练数据过度拟合，导致其泛化能力变差的问题。较新的技术支持向量机（SVM）通常比神经网络更受青睐，成为众多BCI选择的分类算法。

LDA和感知器通过选择超平面来分离两类，而被选择的超平面有无数种合适的可能，可以证明在这些超平面中，**选择两类之间距离最大的超平面**能获得最好的泛化能力。SVM就是这样的一个分类器。

![支持向量机](https://raw.githubusercontent.com/MinJoker/ImageHost/main/books/BCIAI/8.jpg "支持向量机")

**线性SVM已经成功用于大量BCI应用**。线性SVM就不能解决问题的情况下，可利用**核技巧**（kernel trick）来有效实现数据的非线性映射，将数据映射到更高维的空间中，使数据线性可分。（BCI中最常用的核是高斯核或径向基函数）

### 集成分类技术

分类的集成方法结合多个分类器的输出，形成一个比任意单一分类器的泛化能力更好的综合分类器。

#### bagging与随机森林

bagging是一种最简单的集成学习方法，可概括如下：

1. 对给定数据集进行有放回抽样（意味着同一个训练集中样本可能重复），产生m个新的训练集；
2. 训练m个分类器，每个分类器对应一个新产生的训练集；
3. 通过m个分类器对新的输入进行分类，选择获得最多“投票”的类别；

考虑 $N^{'} = N$ 这一典型情况（$N^{'}$ 指的是每个训练集的抽样样本数），这样产生的样本集被称作`bootstrap样本`。

随机森林可能是当今最流行的bagging技术，其名字源于它们由许多**决策树分类器**组成。在随机森林中，输入向量首先森林中的每一棵树，每棵树预测一个输出类别，森林选择获得树投票最多的类别作为其输出。

由于随机森林**在具有大量输入变量的大数据集上表现良好**，使得它近年来愈加流行。但是随机森林**在BCI中的应用依然相对较少**。

#### boosting

boosting寻找一系列分类器，这些分类器**给予预测错误的数据点的权重高于预测正确的**，由此可以找到新的分类器，对当前分类器不能很好分类的数据点能进行更好的分类。

boosting与bagging不同之处在于它的**新分类器依赖于之前分类器的表现**。

boosting对解决弱分类器问题（这些弱分类器的表现可能仅比随机的结果好一些）非常有用，可以**基于弱分类器构建出强分类器**，以提高准确度。

或许最有名的boosting算法是`AdaBoost`。

### 多分类

上述讨论的分类器都是二分类的，而在BCI应用中，期望的输出信号个数通常大于二，这就需要使用多分类方法。

多分类也可以通过二分类器结合的方法得到：
- 训练若干个二分类器，并使用多数投票机制，每个二分类器用于一个两类组合；
- 采取一对多的策略，对于每一类，训练一个独立的分类器来分离该类数据和其他类别的数据；

#### 最近邻

最近邻（NN）是最简单的多分类技术之一，输入被简单指定为其最近邻的类别。

NN分类存在的一个问题是它对噪声和异常值很敏感，这种技术可以通过使用**k-最近邻**（k-NN）来变得更加稳健。在k-NN中，输入被指定为k个最近邻中最普遍的一种类别。

![k-最近邻](https://raw.githubusercontent.com/MinJoker/ImageHost/main/books/BCIAI/9.jpg "k-最近邻")

k-NN技术存在的一个潜在问题是它偏向于训练集中最多样本所属的类别，其有一种变式，把距离和类别同时纳入考虑，可以解决这一问题。

#### 学习矢量量化

学习矢量量化（LVQ）中，分类基于标签 $Y_i\in [1,\cdots ,N_y]$ 标记的特征向量 $\{m_i, Y_i \}_{i=1}^N$（也称为码本向量，lodebook vector）构成的小集合。新样本的分类通过将与它距离最近的码本向量 $m_k$ 的标签 $Y_k$ 赋予样本实现。

LVQ中的每个码本向量的贡献是相等的，但在BCI中更常见的情况是需要对不同特征作加权处理，这就需要使用LVQ的一种优化，即**区分敏感LVQ**（DSLVQ），在计算“距离”时进行加权。

#### 朴素贝叶斯分类器

朴素贝叶斯分类器是一种基于强独立性（朴素）假设贝叶斯准则的概率分类器。假设目标是要根据输入计算得到的大量特征 $F_1, F_2, \cdots , F_n$ 确定特定输入的所属类别（从 $N$ 种可能的类别中选择）。实现这一目标的一种方式是选择具有最大后验概率的类别 $i$ ：

$$
\begin{aligned}
P(C = i \vert F_1, F_2, \cdots , F_n) & = \frac{P(C=i) P(F_1, F_2, \cdots , F_n | C=i)}{P(F_1, F_2, \cdots , F_n)} \\[4ex]
& = \frac{P(C=i) P(F_1 | C=i)P(F_2 | C=i)\cdots P(F_n | C=i)}{P(F_1,F_2,\cdots ,F_n)} \\[4ex]
& \propto P(C=i) P(F_1 | C=i)P(F_2 | C=i)\cdots P(F_n | C=i)
\end{aligned}
$$

### 分类性能的评估

#### 混淆矩阵与ROC曲线：

对于 $N_y \times N_y$ 的混淆矩阵 $M$ ，行表示真实的类标签，列表示分类器的输出。$M$ 的对角元素表示正确分类的样本，非对角元素 $M_{ij}$ 给出了有多少类 $i$ 的样本被误分类为 $j$ 。

两类问题的混淆矩阵如下：

$$
\begin{array}{lll}
{\qquad}&{正}&{负}\\
\hline
{正}&{真阳性（TP）}&{假阴性（FN）}\\
{负}&{假阳性（FP）}&{真阴性（TN）}
\end{array}
$$

ROC曲线（“受试者操作特征”曲线）是真阳性比例和假阳性比例的对比图，能**反映敏感性与特异性之间的关系**。横坐标为假阳性率（$1-$特异性），纵坐标为真阳性率（敏感度）。

![ROC空间](https://raw.githubusercontent.com/MinJoker/ImageHost/main/books/BCIAI/10.jpg "ROC空间")

根据曲线的位置，把整个图分成两部分，曲线下方部分的面积（AUC）越大，表示预测准确性越高；曲线越接近左上角，预测准确性越高。

![ROC曲线](https://raw.githubusercontent.com/MinJoker/ImageHost/main/books/BCIAI/11.jpg "ROC曲线")

#### 分类正确率与Kappa系数

分类正确率（ACC）定义为正确分类的样本数与样本总数的比值。定义错误率 $err = 1 - ACC$ 。当每一类的样本数目相同时，机会水平（随机） $ACC_0 = 1/N_Y$ ,其中 $N_Y$ 表示类别数。

kappa系数定义如下：

$$
\kappa = \frac{ACC - ACC_0}{1 - ACC_0}
$$

kappa系数与每类样本的数目和类别数无关。$\kappa = 0$ 意味着机会水平的表现，$\kappa = 1$ 意味着最好的分类，$\kappa < 0$ 意味着分类性能差于机会水平。

#### 信息传输率

为比较BCI性能，同时考虑BCI的正确率和速度是非常重要，可以利用信息论的概念，根据比特率或者信息传输率（ITR）来量化BCI的性能。

$$
B = log_2 (N) + P log_2 (P) + (1-P) log_2 (1-P) / (N-1)
$$

现实情况下虽然不一定总能满足推导 $B$ 所做得假设条件，但是 $B$ 提供了一个能够获得的性能上限。

#### 交叉验证

交叉验证用于对错误率 $err$ 的估计。一种方法是将给定的输入数据集简单地划分为两个子集，一个子集用于训练，另一个子集用于测试（hand out 方法）。但是这种方法对数据怎样划分很敏感。

一种更复杂的方法是**K折交叉验证**：将数据集划分为K个维数大致相同的子集，其中K-1个子集用于训练分类器，剩下的子集用于训练。对分类器进行K次训练和测试，产生K个不同的错误率，总错误率可以通过求平均得到：

$$
err = \frac{1}{K} \sum_{k=1}^K err_k
$$

在很多的应用中，一般将数据集划分为三个子集：一个是用于确定分类器参数的训练子集，一个是用于调整分类器参数的验证子集，一个是用于报告优化分类器性能的测试子集。虽然这些过程计算量大，但它们对于提高分类器的泛化能力起着重要作用。

## 回归方法

### 线性回归

线性回归假设产生数据的向量函数是线性的。为了更好地说明线性回归，考虑输入 $u$ 是 $K$ 维向量、输出 $v$ 是标量值的特殊情况，于是输出由线性函数给出：

$$
v = \sum_{i=1}^K w_i u_i = w^T u
$$

上式中，$w$ 是需要由训练数据来确定的“权”向量或线性滤波器。线性最小二乘回归寻找能减少所有训练样本的输出误差平方和的权向量 $w$：

$$
\begin{aligned}
E(w) &= \sum_m (d^m - v^m) ^2 \\[2ex]
&= \Vert d-Uw \Vert ^2
\end{aligned}
$$

上式中，$d$ 是训练输出向量，$U$ 是输入矩阵，矩阵的行是来自训练集的输入向量 $u$。为减小误差，对关于 $w$ 的 $E$ 求导，并将求导结果置为 $0$，得到：

$$
2 \cdot U^T (d-Uw) = 0
$$

$$
U^T U w = U^T d
$$

$$
w = (U^T U) ^{-1} U^T d
$$

估计权向量的上述方法有时也被称为广义逆法（矩阵 $(U^T U)^{-1} U^T$ 是“伪逆”的）。

**线性回归已经被证明在很多侵入式BCI中非常有效**。它速度快且易于计算，主要缺点是在某些应用中对模型过于简化，此外，它也没有在输出中提供任何关于不确定性的估计。

### 神经网络与反向传播算法

神经网络是用于**非线性函数逼近**的流行算法。

我们在讨论分类技术的时候涉及了感知器，它是一种神经网络。阈值函数对分类是有效的，但对非线性回归是无效的，对回归来说，流行的选择是`sigmoid`输出函数：

$$
v = g(w^T u)
$$

$$
g(x) = \frac{1}{1+e^{-\beta x}}
$$

sigmoid函数可以看做是阈值函数更平滑的版本：它将输入压缩到0~1之间，用参数 $\beta$ 控制函数的斜率（$\beta$ 值越大，sigmoid函数越接近阈值函数）。sigmoid函数容易求导，这在推导反向传播学习规则时将变得很重要。

![sigmoid函数](https://raw.githubusercontent.com/MinJoker/ImageHost/main/books/BCIAI/12.jpg "sigmoid函数")

对非线性回归来说，我们感兴趣的是**包括多层神经元的网络**，网络中上一层的输出作为下一层神经元的输入。最常见的一种多层网络是包括一个输入层、一个“隐藏”层、一个输出层的三层网络，至少在理论上已经证明，这种网络**能够通过隐藏层中足够多的神经元逼近任何非线性函数**。

假定有一个由sigmoid神经元构成的三层网络，矩阵 $V$ 表示输入层到隐藏层的权重，矩阵 $W$ 表示隐藏层到输出层的权重。输出层中第 $i$ 个神经元的输出可表示为：

$$
v_i = g(\sum_j W_{ji} g(\sum_k V_{kj} u_k))
$$

![三层神经网络图示](https://raw.githubusercontent.com/MinJoker/ImageHost/main/books/BCIAI/13.jpg "三层神经网络图示")

和线性回归一样，其目标也是要减少训练数据期望的输出向量与由网络产生的实际输出向量之间的误差。对训练中每个输入，其误差如下：

$$
E(W,V) = \frac{1}{2} \sum_i (d_i - v_i) ^2
$$

这里需要注意两点：

1. 由于sigmoid非线性函数的存在，不能像线性回归那样直接通过将 $E$ 的导数置零来导出权重的解析表达式；
2. 只知道输出层的误差（上式）；

因此需要反向传播误差信息到网络的更低层，以便能知道它们对输出误差的贡献，成比例的修正权重（“信任分配”问题）。**反向传播算法**可以作为这两个问题的解决方案。

??? note "反向传播算法"

    反向传播算法试图通过让权重 $W$ 和 $V$ 的函数 $E$ 的梯度下降来减小输出误差函数 $E(W,V)$ 。这意味着更新与 $-\frac{\partial E}{\partial W}$ 和 $-\frac{\partial E}{\partial V}$ 成正比的权重，直到权重的变化变小，表明已经达到了误差函数的局部最小值。利用链式法则能够很容易地导出用于更新权重 $W$ 外层的表达式：

    $$
    W_{ji} \leftarrow W_{ji} - \epsilon \frac{\mathrm{d} E}{\mathrm{d} W_{ji}}
    $$

    $$
    \frac{\mathrm{d} E}{\mathrm{d} W_{ji}} = -(d_i - v_i) g \prime (\sum_m W_{mi}x_m) x_j
    $$

    上式中，$\leftarrow$ 表示左边的表达式被右边的表达式替代，$\epsilon$ 是“学习率”（0~1之间的正数），$g\prime$ 是sigmoid函数 $g$ 的导数，$x_j$ 是隐藏层神经元 $j$ 的输出：$x_j = g(\displaystyle{\sum_k} v_{kj}u_k)$ 。

    用于更新权重 $V$ 内层的表达式也能利用链式法则获得：

    $$
    V_{kj} \leftarrow V_{kj} - \epsilon \frac{\mathrm{d} E}{\mathrm{d} V_{kj}}
    $$

    $$
    \frac{\mathrm{d} E}{\mathrm{d} V_{kj}} = \frac{\mathrm{d} E}{\mathrm{d} x_j} \cdot \frac{\mathrm{d} x_j}{\mathrm{d} V_{kj}}
    $$

    $$
    \frac{\mathrm{d} E}{\mathrm{d} V_{kj}} = [-\sum_i (d_i - v_i) g\prime (\sum_m W_{mi}x_m) W_{ji}] \cdot [g\prime (\sum_n V_{nj}u_n) u_k]
    $$

    可以看到，输出误差 $(d_i - v_i)$ 影响了权重内层的更新，可以在每一层中通过对sigmoid非线性函数求导来适当调整输出误差。误差被反向传播到更低层，因而算法由此得名。

尽管这种网络容易对训练数据过度拟合而导致泛化能力差，这种学习过程还是可以推广到任意数量的层，包括含有很多隐藏层的深层网络。**大多数的BCI应用趋向于使用三层网络**，并使用交叉验证来决定隐藏层神经元的数量。

### 径向基函数网络

考虑前面讨论过的线性回归模型：

$$
v = w^T u
$$

提高这种线性模型功效的一种方式是使用一组 $M$ 维固定的非线性基函数（“特征”）$\varphi_i$ ，它们是由输入向量定义的：

$$
v = w^T \varphi (u)
$$

上式中，$\varphi (u)$ 为 $M$ 维向量 $[\varphi_1(u),\cdots,\varphi_M(u)]^T$ 。

然后可以使用前面讨论的线性回归中所用的方法来估计给定的一组基函数的权向量 $w$ 。如果每个基函数 $\varphi_i$ 仅与到“中心” $\mu_i$ 的径向距离（如欧氏距离）有关，那么 $\mu_i(u) = f(\Vert u-\mu_i \Vert)$ ，生成的模型称为**径向基函数（RBF）网络**。

RBF网络可以看做三层神经元网络，其中输入层与隐藏层之间的连接存储着均值 $\mu_i$ ，隐藏层神经元的输出为 $\mu_i(u)$ ，网络的输出 $v$ 是**隐藏层神经元输出的线性加权组合**：

$$
v = \sum_{i=1}^M w_i \varphi_i(u) = w^T \varphi(u)
$$

常用的基函数是“高斯核”：

$$
\varphi_i(u) = \mathrm{exp} (-\Vert u-u_i \Vert ^2 / 2\sigma^2)
$$

![RBF网络图示](https://raw.githubusercontent.com/MinJoker/ImageHost/main/books/BCIAI/14.jpg "RBF网络图示")

### 高斯过程

前面讨论的回归方法的主要缺点是它们无法估计输出预测值的置信度。**高斯过程（GP）回归**为其输出结果**提供了不确定性的测量**。该方法还具有**非参数化**的优点，即该模型结构能随着数据的变化而变化，而不会保持固定，以适应数据的复杂度。

??? note "高斯过程实例"

    假设我们使用前面讨论RBF网络时使用的模型：

    $$
    v = w^T \varphi(u)
    $$

    然而，现在采用一种概率方法，假设 $w$ 服从以下分布：

    $$
    p(w) = G(w \vert 0, \sigma^2 I)
    $$

    上式中，$G$ 表示平均值为 $0$ ，协方差为 $\sigma^2 I$ 的多变量高斯分布。

    给定一组输入数据点 $u_1, \cdots, u_N$ ，考虑输出值 $v(u_1), \cdots, v(u_N)$ 的联合分布。用向量 $\nu$ 表示 $[v(u_1), \cdots, v(u_N)]^T$ ，公式可重写为：

    $$
    \nu = \varPhi w
    $$

    上式中，$\varPhi$ 为矩阵，其元素为 $\varPhi_{ji} = \varphi_i(u_j)$ 。

    由于 $\nu$ 是高斯分布变量（由 $w$ 的元素给出）的线性组合，因此 $\nu$ 也是服从高斯分布的，它可由下面公式给出的均值和协方差来完全定义：

    $$
    \mathrm{mean} (v) = E(\varPhi w) = \varPhi E(w) = 0
    $$

    $$
    \mathrm{cov} (v) = E(v v^T) = \varPhi E(w w^T) \varPhi^T = \sigma^2 \varPhi \varPhi^T = K
    $$

    上式中，$K$ 称为Gram矩阵，其元素为：

    $$
    K_{ij} = k(u_i,u_j) = \sigma^2 \varphi(u_i)^T \varphi(u_j)
    $$

    函数 $k(u_i,u_j)$ 称为核函数。

    以上关于 $\nu$ 的模型是高斯过程的一个实例，它可以定义为关于函数 $v(u)$ 的概率分布，这使得对任意 $N$ 值关于 $v(u_1), \cdots, v(u_N)$ 的联合分布都是高斯分布。在对函数 $v(u)$ 无任何先验知识的条件下，假设其均值为 $0$ ，这意味着高斯过程完全由协方差函数 $K$ 或等效的核函数 $k(u_i,u_j)$ 确定。

上述例子中的核函数可以通过假设定义于输入 $u$ 的基函数 $\varphi_i$ 获取，但核函数也可以直接定义。例如，可以使用由下式给出的高斯核函数：

$$
k(u_i,u_j) = \mathrm{exp} (-\Vert u-u_i \Vert ^2 / 2\sigma^2)
$$

**核函数**可以看做对两个输入之间相似度的测量，它影响函数的平滑度等属性。一般来说，任意函数都可以用作核函数，只要对于任意输入其相关矩阵 $K$ 是半正定的。**核函数的选择需要由应用来决定，而高斯核函数是较为普遍的选择**。

为了将高斯过程用于回归，需要为新的输入 $u_{N+1}$ 预测其输出 $v_{N+1}$ ，给定的训练数据由向量 $\nu_N = [v_1, \cdots, v_N]^T$ 表示的输出组成，相应的输入为 $u_1, \cdots, u_N$ 。期望的后验分布 $p(v_{N+1} \vert \nu_N, u_1, \cdots, u_N)$ 也是高斯分布，其均值和协方差如下：

$$
\mathrm{mean} = k^T C_N^{-1} \nu_N
$$

$$
\mathrm{variance} = c - k^T C_N^{-1} k
$$

上式中，$k$ 是包含元素 $k(u_1,u_{N+1}), i=1,\cdots,N$（$k$ 实质上测量每个训练输入与新输入之间的相似度）的向量，$C_N$ 是协方差矩阵，当 $i\not=j$ 时，其元素由 $C_N(u_i,u_j) = k(u_i,u_j)$ 表示，当 $i=j$ 时，其元素由 $k(u_i,u_j) + \lambda$ 表示，式中 $i,j=1,\cdots,N$（这里 $\lambda$ 是与输出上的噪声相关的参数）。标量值 $c$ 定义为 $c=k(u_{N+1},u_{N+1})+\lambda$ 。

从这些公式可以看出，输出 $v_{N+1}$ 的后验分布不仅取决于过去的训练输入和输出（通过 $C_N$ 与 $\nu_N$），也取决于新的输入（通过 $k$ 与 $c$）。

该模型具有一个优良特性：与训练数据密集的区域相比，在训练数据不足或者不存在的区域，输出预测值的方差较大，反映出更大的不确定性。这个特性对于**需要控制自动设备的BCI应用**非常有用，当预测值的不确定性较高时，BCI可以选择不执行命令，以避免发生可能的灾难性事故。一些BCI常常不具备这样的能力，它们使用无法提供输出不确定性估计值的回归模型，例如神经网络。